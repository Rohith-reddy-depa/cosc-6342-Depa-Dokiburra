{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-v0E_S-mQP"
   },
   "source": [
    "<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n",
    "\n",
    "# Handwritten Text Recognition using TensorFlow 2.x\n",
    "\n",
    "This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMty1YwuWHpN"
   },
   "source": [
    "## 1 Localhost Environment\n",
    "\n",
    "We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39blvPTPQJpt"
   },
   "source": [
    "### 1.1 Datasets\n",
    "\n",
    "The datasets that you can use:\n",
    "\n",
    "a. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n",
    "\n",
    "b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n",
    "\n",
    "c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n",
    "\n",
    "d. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n",
    "\n",
    "e. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVBGMLifWQwl"
   },
   "source": [
    "### 1.2 Raw folder\n",
    "\n",
    "On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n",
    "\n",
    "```\n",
    ".\n",
    "├── raw\n",
    "│   ├── bentham\n",
    "│   │   ├── BenthamDatasetR0-GT\n",
    "│   │   └── BenthamDatasetR0-Images\n",
    "│   ├── iam\n",
    "│   │   ├── ascii\n",
    "│   │   ├── forms\n",
    "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
    "│   │   ├── lines\n",
    "│   │   └── xml\n",
    "│   ├── rimes\n",
    "│   │   ├── eval_2011\n",
    "│   │   ├── eval_2011_annotated.xml\n",
    "│   │   ├── training_2011\n",
    "│   │   └── training_2011.xml\n",
    "│   ├── saintgall\n",
    "│   │   ├── data\n",
    "│   │   ├── ground_truth\n",
    "│   │   ├── README.txt\n",
    "│   │   └── sets\n",
    "│   └── washington\n",
    "│       ├── data\n",
    "│       ├── ground_truth\n",
    "│       ├── README.txt\n",
    "│       └── sets\n",
    "└── src\n",
    "    ├── data\n",
    "    │   ├── evaluation.py\n",
    "    │   ├── generator.py\n",
    "    │   ├── preproc.py\n",
    "    │   ├── reader.py\n",
    "    │   ├── similar_error_analysis.py\n",
    "    ├── main.py\n",
    "    ├── network\n",
    "    │   ├── architecture.py\n",
    "    │   ├── layers.py\n",
    "    │   ├── model.py\n",
    "    └── tutorial.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "After that, create virtual environment and install the dependencies with python 3 and pip:\n",
    "\n",
    "> ```python -m venv .venv && source .venv/bin/activate```\n",
    "\n",
    "> ```pip install -r requirements.txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyLRbAwsWSYA"
   },
   "source": [
    "### 1.3 HDF5 files\n",
    "\n",
    "Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n",
    "\n",
    "> ```python main.py --source=<DATASET_NAME> --transform```\n",
    "\n",
    "Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n",
    "\n",
    "\n",
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── bentham.hdf5\n",
    "│   ├── iam.hdf5\n",
    "│   ├── rimes.hdf5\n",
    "│   ├── saintgall.hdf5\n",
    "│   └── washington.hdf5\n",
    "├── raw\n",
    "│   ├── bentham\n",
    "│   │   ├── BenthamDatasetR0-GT\n",
    "│   │   └── BenthamDatasetR0-Images\n",
    "│   ├── iam\n",
    "│   │   ├── ascii\n",
    "│   │   ├── forms\n",
    "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
    "│   │   ├── lines\n",
    "│   │   └── xml\n",
    "│   ├── rimes\n",
    "│   │   ├── eval_2011\n",
    "│   │   ├── eval_2011_annotated.xml\n",
    "│   │   ├── training_2011\n",
    "│   │   └── training_2011.xml\n",
    "│   ├── saintgall\n",
    "│   │   ├── data\n",
    "│   │   ├── ground_truth\n",
    "│   │   ├── README.txt\n",
    "│   │   └── sets\n",
    "│   └── washington\n",
    "│       ├── data\n",
    "│       ├── ground_truth\n",
    "│       ├── README.txt\n",
    "│       └── sets\n",
    "└── src\n",
    "    ├── data\n",
    "    │   ├── evaluation.py\n",
    "    │   ├── generator.py\n",
    "    │   ├── preproc.py\n",
    "    │   ├── reader.py\n",
    "    │   ├── similar_error_analysis.py\n",
    "    ├── main.py\n",
    "    ├── network\n",
    "    │   ├── architecture.py\n",
    "    │   ├── layers.py\n",
    "    │   ├── model.py\n",
    "    └── tutorial.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "Then upload the **data** and **src** folders in the same directory in your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jydsAcWgWVth"
   },
   "source": [
    "## 2 Google Drive Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk3e7YJiXzSl"
   },
   "source": [
    "### 2.1 TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7twXyNGXtbJ"
   },
   "source": [
    "Make sure the jupyter notebook is using GPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHw4tODULT1Z"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMg-B5PH9h3r"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "\n",
    "print(\"Found GPU at: {}\".format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyMv5wyDXxqc"
   },
   "source": [
    "### 2.2 Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5gj6qwoX9W3"
   },
   "source": [
    "Mount your Google Drive partition.\n",
    "\n",
    "**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACQn1iBF9k9O"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"./gdrive\", force_remount=True)\n",
    "\n",
    "%cd \"./gdrive/My Drive/Colab Notebooks/handwritten-text-recognition/src/\"\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwogUA8RZAyp"
   },
   "source": [
    "After mount, you can see the list os files in the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fj7fSngY1IX"
   },
   "source": [
    "## 3 Set Python Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Q4cOlWhNl3"
   },
   "source": [
    "### 3.1 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvqL2Eq5ZUc7"
   },
   "source": [
    "First, let's define our environment variables.\n",
    "\n",
    "Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n",
    "\n",
    "* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n",
    "\n",
    "* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n",
    "\n",
    "* **epochs**: number of epochs\n",
    "\n",
    "* **batch_size**: number size of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "_Qpr3drnGMWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "ARCH = \"flor\"\n",
    "\n",
    "IMG_SIZE = (1642, 6986, 1)\n",
    "DATA_ROOT_PATH = \"..//data\"\n",
    "IMAGES_PATH = os.path.join(DATA_ROOT_PATH, \"images\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "EsU8fQR2jMrP"
   },
   "outputs": [],
   "source": [
    "vocab = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"hindi_vocab.txt\"),encoding=\"latin1\") as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "idx_to_vocab = {i:value.strip() for i, value in enumerate(vocab)}\n",
    "vocab_to_idx = {value:key for key, value in idx_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à¤µà¤¾à¤°à¥\\x8dà¤·à¥\\x8dà¤£à¥\\x87à¤¯'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4436"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_idx['à¤¸à¤¾à¤®à¤°à¤¿à¤\\x95']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "DrpcgsidjMrP"
   },
   "outputs": [],
   "source": [
    "train_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_train.txt\"), encoding=\"latin1\") as f:\n",
    "  train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HindiSeg/train/10/259/1.jpg 10800\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HindiSeg/train/10/259/1.jpg', '10800']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_data = train_data[0].split()\n",
    "curr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_val.txt\"), encoding=\"latin1\") as f:\n",
    "  valid_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFextshOhTKr"
   },
   "source": [
    "### 3.2 DataGenerator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfZ1mfvsanu1"
   },
   "source": [
    "The second class is **DataGenerator()**, responsible for:\n",
    "\n",
    "* Load the dataset partitions (train, valid, test);\n",
    "\n",
    "* Manager batchs for train/validation/test process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "8k9vpNzMIAi2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, img_size, batch_size, mode=\"TRAIN\"):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        end = (i+1) * self.batch_size\n",
    "        \n",
    "        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1))\n",
    "        batch_labels = np.zeros((self.batch_size, 1))\n",
    "        \n",
    "        for ii, df_index in enumerate(range(start, end)):\n",
    "            curr_data = self.data[ii].split()\n",
    "            curr_img_path = curr_data[0]\n",
    "            #curr_label = idx_to_vocab[int(curr_data[1])]\n",
    "            curr_label = int(curr_data[1])\n",
    "\n",
    "            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "            curr_img = Image.open(curr_img_path)\n",
    "            curr_img = ImageOps.grayscale(curr_img)\n",
    "            \n",
    "            delta_width = self.img_size[1] - curr_img.width\n",
    "            delta_height = self.img_size[0] - curr_img.height\n",
    "            pad_width = delta_width // 2\n",
    "            pad_height = delta_height // 2\n",
    "            padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "            curr_img = ImageOps.expand(curr_img, padding)\n",
    "            \n",
    "            curr_img = img_to_array(curr_img)\n",
    "            img_shape = curr_img.shape\n",
    "            \n",
    "            #curr_img = tf.image.resize(curr_img, (self.img_size[0], self.img_size[1]), method=\"nearest\")\n",
    "            #curr_img = curr_img.numpy().reshape((self.img_size[0], self.img_size[1]))\n",
    "            batch_images[ii, :, :,:] = curr_img / 255.\n",
    "            batch_labels[ii, :] = curr_label\n",
    "                    \n",
    "        if self.mode == \"TRAIN\":\n",
    "          return batch_images, batch_labels\n",
    "        else:\n",
    "          return batch_images\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.data) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "uxhnYiJUjMrR"
   },
   "outputs": [],
   "source": [
    "train_datagen = DataGen(train_data, IMG_SIZE, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OdgNLK0hYAA"
   },
   "source": [
    "### 3.3 HTRModel Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHktk8AFcnKy"
   },
   "source": [
    "The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n",
    "\n",
    "* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n",
    "\n",
    "* Save and load model;\n",
    "\n",
    "* Load weights in the models (train/infer);\n",
    "\n",
    "* Make Train/Predict process using *generator*.\n",
    "\n",
    "To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "nV0GreStISTR"
   },
   "outputs": [],
   "source": [
    "from network.model import HTRModel\n",
    "\n",
    "# create and compile HTRModel\n",
    "model = HTRModel(architecture=ARCH,\n",
    "                 input_size=IMG_SIZE,\n",
    "                 vocab_size=11031,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15,\n",
    "                 reduce_factor=0.1)\n",
    "\n",
    "model.compile(learning_rate=0.001)\n",
    "#model.summary(output_path, \"summary.txt\")\n",
    "\n",
    "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
    "#model.load_checkpoint(target=target_path)\n",
    "\n",
    "#callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1fnz0Eugqru"
   },
   "source": [
    "## 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1mLOcqYgsO-"
   },
   "source": [
    "The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P6MSoxCISlD",
    "outputId": "4f21673d-a31a-4a07-b65f-0e0aae4a688d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "# to calculate total and average time per epoch\n",
    "#start_time = datetime.datetime.now()\n",
    "\n",
    "h = model.fit(x=train_datagen,\n",
    "              epochs=EPOCHS,\n",
    "              verbose=1)\n",
    "\n",
    "'''\n",
    "total_time = datetime.datetime.now() - start_time\n",
    "\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "min_val_loss = min(val_loss)\n",
    "min_val_loss_i = val_loss.index(min_val_loss)\n",
    "\n",
    "time_epoch = (total_time / len(loss))\n",
    "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
    "\n",
    "t_corpus = \"\\n\".join([\n",
    "    f\"Total train images:      {dtgen.size['train']}\",\n",
    "    f\"Total validation images: {dtgen.size['valid']}\",\n",
    "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
    "    f\"Total time:              {total_time}\",\n",
    "    f\"Time per epoch:          {time_epoch}\",\n",
    "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
    "    f\"Total epochs:            {len(loss)}\",\n",
    "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
    "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
    "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
    "    lg.write(t_corpus)\n",
    "    print(t_corpus)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datagen = DataGen(train_data, IMG_SIZE, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(valid_datagen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Predict\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "CTC Decode\n",
      "1/1 [==============================] - 41s 41s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npredicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\\nground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import preproc as pp\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "#start_time = datetime.datetime.now()\n",
    "\n",
    "# predict() function will return the predicts with the probabilities\n",
    "predicts, _ = model.predict(X,\n",
    "                            verbose=1)\n",
    "\n",
    "# decode to string\n",
    "'''\n",
    "predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
    "ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([0.], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0',\n",
       " 1: '1',\n",
       " 2: '2',\n",
       " 3: '3',\n",
       " 4: '4',\n",
       " 5: '5',\n",
       " 6: '6',\n",
       " 7: '7',\n",
       " 8: '8',\n",
       " 9: '9',\n",
       " 10: '10',\n",
       " 11: '11',\n",
       " 12: '12',\n",
       " 13: '13',\n",
       " 14: '14',\n",
       " 15: '15',\n",
       " 16: '16',\n",
       " 17: '17',\n",
       " 18: '18',\n",
       " 19: '19',\n",
       " 20: '20',\n",
       " 21: '21',\n",
       " 22: '22',\n",
       " 23: '23',\n",
       " 24: '24',\n",
       " 25: '25',\n",
       " 26: '26',\n",
       " 27: '27',\n",
       " 28: '28',\n",
       " 29: '29',\n",
       " 30: '30',\n",
       " 31: '31',\n",
       " 32: '32',\n",
       " 33: '33',\n",
       " 34: '34',\n",
       " 35: '35',\n",
       " 36: '36',\n",
       " 37: '37',\n",
       " 38: '38',\n",
       " 39: '39',\n",
       " 40: '40',\n",
       " 41: '41',\n",
       " 42: '42',\n",
       " 43: '43',\n",
       " 44: '44',\n",
       " 45: '45',\n",
       " 46: '46',\n",
       " 47: '47',\n",
       " 48: '48',\n",
       " 49: '49',\n",
       " 50: '50',\n",
       " 51: '51',\n",
       " 52: '52',\n",
       " 53: '53',\n",
       " 54: '54',\n",
       " 55: '55',\n",
       " 56: '56',\n",
       " 57: '57',\n",
       " 58: '58',\n",
       " 59: '59',\n",
       " 60: '60',\n",
       " 61: '61',\n",
       " 62: '62',\n",
       " 63: '63',\n",
       " 64: '64',\n",
       " 65: '65',\n",
       " 66: '66',\n",
       " 67: '67',\n",
       " 68: '68',\n",
       " 69: '69',\n",
       " 70: '70',\n",
       " 71: '71',\n",
       " 72: '72',\n",
       " 73: '73',\n",
       " 74: '74',\n",
       " 75: '75',\n",
       " 76: '76',\n",
       " 77: '77',\n",
       " 78: '78',\n",
       " 79: '79',\n",
       " 80: '80',\n",
       " 81: '81',\n",
       " 82: '82',\n",
       " 83: '83',\n",
       " 84: '84',\n",
       " 85: '85',\n",
       " 86: '86',\n",
       " 87: '87',\n",
       " 88: '88',\n",
       " 89: '89',\n",
       " 90: '90',\n",
       " 91: '91',\n",
       " 92: '92',\n",
       " 93: '93',\n",
       " 94: '94',\n",
       " 95: '95',\n",
       " 96: '96',\n",
       " 97: '97',\n",
       " 98: '98',\n",
       " 99: '99',\n",
       " 100: '100',\n",
       " 101: '101',\n",
       " 102: '102',\n",
       " 103: '103',\n",
       " 104: '104',\n",
       " 105: '105',\n",
       " 106: '106',\n",
       " 107: '107',\n",
       " 108: '108',\n",
       " 109: '109',\n",
       " 110: '110',\n",
       " 111: '111',\n",
       " 112: '112',\n",
       " 113: '113',\n",
       " 114: '114',\n",
       " 115: '115',\n",
       " 116: '116',\n",
       " 117: '117',\n",
       " 118: '118',\n",
       " 119: '119',\n",
       " 120: '120',\n",
       " 121: '121',\n",
       " 122: '122',\n",
       " 123: '123',\n",
       " 124: '124',\n",
       " 125: '125',\n",
       " 126: '126',\n",
       " 127: '127',\n",
       " 128: '128',\n",
       " 129: '129',\n",
       " 130: '130',\n",
       " 131: '131',\n",
       " 132: '132',\n",
       " 133: '133',\n",
       " 134: '134',\n",
       " 135: '135',\n",
       " 136: '136',\n",
       " 137: '137',\n",
       " 138: '138',\n",
       " 139: '139',\n",
       " 140: '140',\n",
       " 141: '141',\n",
       " 142: '142',\n",
       " 143: '143',\n",
       " 144: '144',\n",
       " 145: '145',\n",
       " 146: '146',\n",
       " 147: '147',\n",
       " 148: '148',\n",
       " 149: '149',\n",
       " 150: '150',\n",
       " 151: '151',\n",
       " 152: '152',\n",
       " 153: '153',\n",
       " 154: '154',\n",
       " 155: '155',\n",
       " 156: '156',\n",
       " 157: '157',\n",
       " 158: '158',\n",
       " 159: '159',\n",
       " 160: '160',\n",
       " 161: '161',\n",
       " 162: '162',\n",
       " 163: '163',\n",
       " 164: '165',\n",
       " 165: '165',\n",
       " 166: '166',\n",
       " 167: '167',\n",
       " 168: '168',\n",
       " 169: '169',\n",
       " 170: '170',\n",
       " 171: '171',\n",
       " 172: '172',\n",
       " 173: '173',\n",
       " 174: '174',\n",
       " 175: '175',\n",
       " 176: '176',\n",
       " 177: '177',\n",
       " 178: '178',\n",
       " 179: '179',\n",
       " 180: '180',\n",
       " 181: '181',\n",
       " 182: '182',\n",
       " 183: '183',\n",
       " 184: '184',\n",
       " 185: '185',\n",
       " 186: '186',\n",
       " 187: '187',\n",
       " 188: '188',\n",
       " 189: '189',\n",
       " 190: '190',\n",
       " 191: '191',\n",
       " 192: '192',\n",
       " 193: '193',\n",
       " 194: '194',\n",
       " 195: '195',\n",
       " 196: '196',\n",
       " 197: '197',\n",
       " 198: '198',\n",
       " 199: '199',\n",
       " 200: '200',\n",
       " 201: '201',\n",
       " 202: '202',\n",
       " 203: '203',\n",
       " 204: '204',\n",
       " 205: '205',\n",
       " 206: '206',\n",
       " 207: '207',\n",
       " 208: '208',\n",
       " 209: '209',\n",
       " 210: '210',\n",
       " 211: '211',\n",
       " 212: '212',\n",
       " 213: '213',\n",
       " 214: '214',\n",
       " 215: '215',\n",
       " 216: '216',\n",
       " 217: '217',\n",
       " 218: '218',\n",
       " 219: '219',\n",
       " 220: '220',\n",
       " 221: '221',\n",
       " 222: '222',\n",
       " 223: '223',\n",
       " 224: '224',\n",
       " 225: '225',\n",
       " 226: '226',\n",
       " 227: '227',\n",
       " 228: '228',\n",
       " 229: '229',\n",
       " 230: '230',\n",
       " 231: '231',\n",
       " 232: '232',\n",
       " 233: '233',\n",
       " 234: '234',\n",
       " 235: '235',\n",
       " 236: '236',\n",
       " 237: '237',\n",
       " 238: '238',\n",
       " 239: '239',\n",
       " 240: '240',\n",
       " 241: '241',\n",
       " 242: '242',\n",
       " 243: '243',\n",
       " 244: '244',\n",
       " 245: '245',\n",
       " 246: '246',\n",
       " 247: '247',\n",
       " 248: '248',\n",
       " 249: '249',\n",
       " 250: '250',\n",
       " 251: '251',\n",
       " 252: '252',\n",
       " 253: '253',\n",
       " 254: '254',\n",
       " 255: '255',\n",
       " 256: '256',\n",
       " 257: '257',\n",
       " 258: '258',\n",
       " 259: '259',\n",
       " 260: '260',\n",
       " 261: '261',\n",
       " 262: '262',\n",
       " 263: '263',\n",
       " 264: '264',\n",
       " 265: '265',\n",
       " 266: '266',\n",
       " 267: '267',\n",
       " 268: '268',\n",
       " 269: '269',\n",
       " 270: '270',\n",
       " 271: '271',\n",
       " 272: '272',\n",
       " 273: '273',\n",
       " 274: '274',\n",
       " 275: '275',\n",
       " 276: '276',\n",
       " 277: '277',\n",
       " 278: '278',\n",
       " 279: '279',\n",
       " 280: '280',\n",
       " 281: '281',\n",
       " 282: '282',\n",
       " 283: '283',\n",
       " 284: '284',\n",
       " 285: '285',\n",
       " 286: '286',\n",
       " 287: '287',\n",
       " 288: '288',\n",
       " 289: '289',\n",
       " 290: '290',\n",
       " 291: '291',\n",
       " 292: '292',\n",
       " 293: '293',\n",
       " 294: '294',\n",
       " 295: '295',\n",
       " 296: '296',\n",
       " 297: '297',\n",
       " 298: '298',\n",
       " 299: '299',\n",
       " 300: '300',\n",
       " 301: '301',\n",
       " 302: '302',\n",
       " 303: '303',\n",
       " 304: '304',\n",
       " 305: '305',\n",
       " 306: '306',\n",
       " 307: '307',\n",
       " 308: '308',\n",
       " 309: '309',\n",
       " 310: '310',\n",
       " 311: '311',\n",
       " 312: '312',\n",
       " 313: '313',\n",
       " 314: '314',\n",
       " 315: '315',\n",
       " 316: '316',\n",
       " 317: '317',\n",
       " 318: '318',\n",
       " 319: '319',\n",
       " 320: '320',\n",
       " 321: '321',\n",
       " 322: '322',\n",
       " 323: '323',\n",
       " 324: '324',\n",
       " 325: '325',\n",
       " 326: '326',\n",
       " 327: '327',\n",
       " 328: '328',\n",
       " 329: '329',\n",
       " 330: '330',\n",
       " 331: '331',\n",
       " 332: '332',\n",
       " 333: '333',\n",
       " 334: '334',\n",
       " 335: '335',\n",
       " 336: '336',\n",
       " 337: '337',\n",
       " 338: '338',\n",
       " 339: '339',\n",
       " 340: '340',\n",
       " 341: '341',\n",
       " 342: '342',\n",
       " 343: '343',\n",
       " 344: '344',\n",
       " 345: '345',\n",
       " 346: '346',\n",
       " 347: '347',\n",
       " 348: '348',\n",
       " 349: '349',\n",
       " 350: '350',\n",
       " 351: '351',\n",
       " 352: '352',\n",
       " 353: '353',\n",
       " 354: '354',\n",
       " 355: '355',\n",
       " 356: '356',\n",
       " 357: '357',\n",
       " 358: '358',\n",
       " 359: '359',\n",
       " 360: '360',\n",
       " 361: '361',\n",
       " 362: '362',\n",
       " 363: '363',\n",
       " 364: '364',\n",
       " 365: '365',\n",
       " 366: '366',\n",
       " 367: '367',\n",
       " 368: '368',\n",
       " 369: '369',\n",
       " 370: '370',\n",
       " 371: '371',\n",
       " 372: '372',\n",
       " 373: '373',\n",
       " 374: '374',\n",
       " 375: '375',\n",
       " 376: '376',\n",
       " 377: '377',\n",
       " 378: '378',\n",
       " 379: '379',\n",
       " 380: '380',\n",
       " 381: '381',\n",
       " 382: '382',\n",
       " 383: '383',\n",
       " 384: '384',\n",
       " 385: '385',\n",
       " 386: '386',\n",
       " 387: '387',\n",
       " 388: '388',\n",
       " 389: '389',\n",
       " 390: '390',\n",
       " 391: '391',\n",
       " 392: '392',\n",
       " 393: '393',\n",
       " 394: '394',\n",
       " 395: '395',\n",
       " 396: '396',\n",
       " 397: '397',\n",
       " 398: '398',\n",
       " 399: '399',\n",
       " 400: '400',\n",
       " 401: '401',\n",
       " 402: '402',\n",
       " 403: '403',\n",
       " 404: '404',\n",
       " 405: '405',\n",
       " 406: '406',\n",
       " 407: '407',\n",
       " 408: '408',\n",
       " 409: '409',\n",
       " 410: '410',\n",
       " 411: '411',\n",
       " 412: '412',\n",
       " 413: '413',\n",
       " 414: '414',\n",
       " 415: '415',\n",
       " 416: '416',\n",
       " 417: '417',\n",
       " 418: '418',\n",
       " 419: '419',\n",
       " 420: '420',\n",
       " 421: '421',\n",
       " 422: '422',\n",
       " 423: '423',\n",
       " 424: '424',\n",
       " 425: '425',\n",
       " 426: '426',\n",
       " 427: '427',\n",
       " 428: '428',\n",
       " 429: '429',\n",
       " 430: '430',\n",
       " 431: '431',\n",
       " 432: '432',\n",
       " 433: '433',\n",
       " 434: '434',\n",
       " 435: '435',\n",
       " 436: '436',\n",
       " 437: '437',\n",
       " 438: '438',\n",
       " 439: '439',\n",
       " 440: '440',\n",
       " 441: '441',\n",
       " 442: '442',\n",
       " 443: '443',\n",
       " 444: '444',\n",
       " 445: '445',\n",
       " 446: '446',\n",
       " 447: '447',\n",
       " 448: '448',\n",
       " 449: '449',\n",
       " 450: '450',\n",
       " 451: '451',\n",
       " 452: '452',\n",
       " 453: '453',\n",
       " 454: '454',\n",
       " 455: '455',\n",
       " 456: '456',\n",
       " 457: '457',\n",
       " 458: '458',\n",
       " 459: '459',\n",
       " 460: '460',\n",
       " 461: '461',\n",
       " 462: '462',\n",
       " 463: '463',\n",
       " 464: '464',\n",
       " 465: '465',\n",
       " 466: '466',\n",
       " 467: '467',\n",
       " 468: '468',\n",
       " 469: '469',\n",
       " 470: '470',\n",
       " 471: '471',\n",
       " 472: '472',\n",
       " 473: '473',\n",
       " 474: '474',\n",
       " 475: '475',\n",
       " 476: '476',\n",
       " 477: '477',\n",
       " 478: '478',\n",
       " 479: '479',\n",
       " 480: '480',\n",
       " 481: '481',\n",
       " 482: '482',\n",
       " 483: '483',\n",
       " 484: '484',\n",
       " 485: '485',\n",
       " 486: '486',\n",
       " 487: '487',\n",
       " 488: '488',\n",
       " 489: '489',\n",
       " 490: '490',\n",
       " 491: '491',\n",
       " 492: '492',\n",
       " 493: '493',\n",
       " 494: '494',\n",
       " 495: '495',\n",
       " 496: '496',\n",
       " 497: '497',\n",
       " 498: '498',\n",
       " 499: '499',\n",
       " 500: '500',\n",
       " 501: '501',\n",
       " 502: '502',\n",
       " 503: '503',\n",
       " 504: '504',\n",
       " 505: '505',\n",
       " 506: '506',\n",
       " 507: '507',\n",
       " 508: '508',\n",
       " 509: '509',\n",
       " 510: '510',\n",
       " 511: '511',\n",
       " 512: '512',\n",
       " 513: '513',\n",
       " 514: '514',\n",
       " 515: '515',\n",
       " 516: '516',\n",
       " 517: '517',\n",
       " 518: '518',\n",
       " 519: '519',\n",
       " 520: '520',\n",
       " 521: '521',\n",
       " 522: '522',\n",
       " 523: '523',\n",
       " 524: '524',\n",
       " 525: '525',\n",
       " 526: '526',\n",
       " 527: '527',\n",
       " 528: '528',\n",
       " 529: '529',\n",
       " 530: '530',\n",
       " 531: '531',\n",
       " 532: '532',\n",
       " 533: '533',\n",
       " 534: '534',\n",
       " 535: '535',\n",
       " 536: '536',\n",
       " 537: '537',\n",
       " 538: '538',\n",
       " 539: '539',\n",
       " 540: '540',\n",
       " 541: '541',\n",
       " 542: '542',\n",
       " 543: '543',\n",
       " 544: '544',\n",
       " 545: '545',\n",
       " 546: '546',\n",
       " 547: '547',\n",
       " 548: '548',\n",
       " 549: '549',\n",
       " 550: '550',\n",
       " 551: '551',\n",
       " 552: '552',\n",
       " 553: '553',\n",
       " 554: '554',\n",
       " 555: '555',\n",
       " 556: '556',\n",
       " 557: '557',\n",
       " 558: '558',\n",
       " 559: '559',\n",
       " 560: '560',\n",
       " 561: '561',\n",
       " 562: '562',\n",
       " 563: '563',\n",
       " 564: '564',\n",
       " 565: '565',\n",
       " 566: '566',\n",
       " 567: '567',\n",
       " 568: '568',\n",
       " 569: '569',\n",
       " 570: '570',\n",
       " 571: '571',\n",
       " 572: '572',\n",
       " 573: '573',\n",
       " 574: '574',\n",
       " 575: '575',\n",
       " 576: '576',\n",
       " 577: '577',\n",
       " 578: '578',\n",
       " 579: '579',\n",
       " 580: '580',\n",
       " 581: '581',\n",
       " 582: '582',\n",
       " 583: '583',\n",
       " 584: '584',\n",
       " 585: '585',\n",
       " 586: '586',\n",
       " 587: '587',\n",
       " 588: '588',\n",
       " 589: '589',\n",
       " 590: '590',\n",
       " 591: '591',\n",
       " 592: '592',\n",
       " 593: '593',\n",
       " 594: '594',\n",
       " 595: '595',\n",
       " 596: '596',\n",
       " 597: '597',\n",
       " 598: '598',\n",
       " 599: '599',\n",
       " 600: '600',\n",
       " 601: '601',\n",
       " 602: '602',\n",
       " 603: '603',\n",
       " 604: '604',\n",
       " 605: '605',\n",
       " 606: '606',\n",
       " 607: '607',\n",
       " 608: '608',\n",
       " 609: '609',\n",
       " 610: '610',\n",
       " 611: '611',\n",
       " 612: '612',\n",
       " 613: '613',\n",
       " 614: '614',\n",
       " 615: '615',\n",
       " 616: '616',\n",
       " 617: '617',\n",
       " 618: '618',\n",
       " 619: '619',\n",
       " 620: '620',\n",
       " 621: '621',\n",
       " 622: '622',\n",
       " 623: '623',\n",
       " 624: '624',\n",
       " 625: '625',\n",
       " 626: '626',\n",
       " 627: '627',\n",
       " 628: '628',\n",
       " 629: '629',\n",
       " 630: '630',\n",
       " 631: '631',\n",
       " 632: '632',\n",
       " 633: '633',\n",
       " 634: '634',\n",
       " 635: '635',\n",
       " 636: '636',\n",
       " 637: '637',\n",
       " 638: '638',\n",
       " 639: '639',\n",
       " 640: '640',\n",
       " 641: '641',\n",
       " 642: '642',\n",
       " 643: '643',\n",
       " 644: '644',\n",
       " 645: '645',\n",
       " 646: '646',\n",
       " 647: '647',\n",
       " 648: '648',\n",
       " 649: '649',\n",
       " 650: '650',\n",
       " 651: '651',\n",
       " 652: '652',\n",
       " 653: '653',\n",
       " 654: '654',\n",
       " 655: '655',\n",
       " 656: '656',\n",
       " 657: '657',\n",
       " 658: '658',\n",
       " 659: '659',\n",
       " 660: '660',\n",
       " 661: '661',\n",
       " 662: '662',\n",
       " 663: '663',\n",
       " 664: '664',\n",
       " 665: '665',\n",
       " 666: '666',\n",
       " 667: '667',\n",
       " 668: '668',\n",
       " 669: '669',\n",
       " 670: '670',\n",
       " 671: '671',\n",
       " 672: '672',\n",
       " 673: '673',\n",
       " 674: '674',\n",
       " 675: '675',\n",
       " 676: '676',\n",
       " 677: '677',\n",
       " 678: '678',\n",
       " 679: '679',\n",
       " 680: '680',\n",
       " 681: '681',\n",
       " 682: '682',\n",
       " 683: '683',\n",
       " 684: '684',\n",
       " 685: '685',\n",
       " 686: '686',\n",
       " 687: '687',\n",
       " 688: '688',\n",
       " 689: '689',\n",
       " 690: '690',\n",
       " 691: '691',\n",
       " 692: '692',\n",
       " 693: '693',\n",
       " 694: '694',\n",
       " 695: '695',\n",
       " 696: '696',\n",
       " 697: '697',\n",
       " 698: '698',\n",
       " 699: '699',\n",
       " 700: '700',\n",
       " 701: '701',\n",
       " 702: '702',\n",
       " 703: '703',\n",
       " 704: '704',\n",
       " 705: '705',\n",
       " 706: '706',\n",
       " 707: '707',\n",
       " 708: '708',\n",
       " 709: '709',\n",
       " 710: '710',\n",
       " 711: '711',\n",
       " 712: '712',\n",
       " 713: '713',\n",
       " 714: '714',\n",
       " 715: '715',\n",
       " 716: '716',\n",
       " 717: '717',\n",
       " 718: '718',\n",
       " 719: '719',\n",
       " 720: '720',\n",
       " 721: '721',\n",
       " 722: '722',\n",
       " 723: '723',\n",
       " 724: '724',\n",
       " 725: '725',\n",
       " 726: '726',\n",
       " 727: '727',\n",
       " 728: '728',\n",
       " 729: '729',\n",
       " 730: '730',\n",
       " 731: '731',\n",
       " 732: '732',\n",
       " 733: '733',\n",
       " 734: '734',\n",
       " 735: '735',\n",
       " 736: '736',\n",
       " 737: '737',\n",
       " 738: '738',\n",
       " 739: '739',\n",
       " 740: '740',\n",
       " 741: '741',\n",
       " 742: '742',\n",
       " 743: '743',\n",
       " 744: '744',\n",
       " 745: '745',\n",
       " 746: '746',\n",
       " 747: '747',\n",
       " 748: '748',\n",
       " 749: '749',\n",
       " 750: '750',\n",
       " 751: '751',\n",
       " 752: '752',\n",
       " 753: '753',\n",
       " 754: '754',\n",
       " 755: '755',\n",
       " 756: '756',\n",
       " 757: '757',\n",
       " 758: '758',\n",
       " 759: '759',\n",
       " 760: '760',\n",
       " 761: '761',\n",
       " 762: '762',\n",
       " 763: '763',\n",
       " 764: '764',\n",
       " 765: '765',\n",
       " 766: '766',\n",
       " 767: '767',\n",
       " 768: '768',\n",
       " 769: '769',\n",
       " 770: '770',\n",
       " 771: '771',\n",
       " 772: '772',\n",
       " 773: '773',\n",
       " 774: '774',\n",
       " 775: '775',\n",
       " 776: '776',\n",
       " 777: '777',\n",
       " 778: '778',\n",
       " 779: '779',\n",
       " 780: '780',\n",
       " 781: '781',\n",
       " 782: '782',\n",
       " 783: '783',\n",
       " 784: '784',\n",
       " 785: '785',\n",
       " 786: '786',\n",
       " 787: '787',\n",
       " 788: '788',\n",
       " 789: '789',\n",
       " 790: '790',\n",
       " 791: '791',\n",
       " 792: '792',\n",
       " 793: '793',\n",
       " 794: '794',\n",
       " 795: '795',\n",
       " 796: '796',\n",
       " 797: '797',\n",
       " 798: '798',\n",
       " 799: '799',\n",
       " 800: '800',\n",
       " 801: '801',\n",
       " 802: '802',\n",
       " 803: '803',\n",
       " 804: '804',\n",
       " 805: '805',\n",
       " 806: '806',\n",
       " 807: '807',\n",
       " 808: '808',\n",
       " 809: '809',\n",
       " 810: '810',\n",
       " 811: '811',\n",
       " 812: '812',\n",
       " 813: '813',\n",
       " 814: '814',\n",
       " 815: '815',\n",
       " 816: '816',\n",
       " 817: '817',\n",
       " 818: '818',\n",
       " 819: '819',\n",
       " 820: '820',\n",
       " 821: '821',\n",
       " 822: '822',\n",
       " 823: '823',\n",
       " 824: '824',\n",
       " 825: '825',\n",
       " 826: '826',\n",
       " 827: '827',\n",
       " 828: '828',\n",
       " 829: '829',\n",
       " 830: '830',\n",
       " 831: '831',\n",
       " 832: '832',\n",
       " 833: '833',\n",
       " 834: '834',\n",
       " 835: '835',\n",
       " 836: '836',\n",
       " 837: '837',\n",
       " 838: '838',\n",
       " 839: '839',\n",
       " 840: '840',\n",
       " 841: '841',\n",
       " 842: '842',\n",
       " 843: '843',\n",
       " 844: '844',\n",
       " 845: '845',\n",
       " 846: '846',\n",
       " 847: '847',\n",
       " 848: '848',\n",
       " 849: '849',\n",
       " 850: '850',\n",
       " 851: '851',\n",
       " 852: '852',\n",
       " 853: '853',\n",
       " 854: '854',\n",
       " 855: '855',\n",
       " 856: '856',\n",
       " 857: '857',\n",
       " 858: '858',\n",
       " 859: '859',\n",
       " 860: '860',\n",
       " 861: '861',\n",
       " 862: '862',\n",
       " 863: '863',\n",
       " 864: '864',\n",
       " 865: '865',\n",
       " 866: '866',\n",
       " 867: '867',\n",
       " 868: '868',\n",
       " 869: '869',\n",
       " 870: '870',\n",
       " 871: '871',\n",
       " 872: '872',\n",
       " 873: '873',\n",
       " 874: '874',\n",
       " 875: '875',\n",
       " 876: '876',\n",
       " 877: '877',\n",
       " 878: '878',\n",
       " 879: '879',\n",
       " 880: '880',\n",
       " 881: '881',\n",
       " 882: '882',\n",
       " 883: '883',\n",
       " 884: '884',\n",
       " 885: '885',\n",
       " 886: '886',\n",
       " 887: '887',\n",
       " 888: '888',\n",
       " 889: '889',\n",
       " 890: '890',\n",
       " 891: '891',\n",
       " 892: '892',\n",
       " 893: '893',\n",
       " 894: '894',\n",
       " 895: '895',\n",
       " 896: '896',\n",
       " 897: '897',\n",
       " 898: '898',\n",
       " 899: '899',\n",
       " 900: '900',\n",
       " 901: '901',\n",
       " 902: '902',\n",
       " 903: '903',\n",
       " 904: '904',\n",
       " 905: '905',\n",
       " 906: '906',\n",
       " 907: '907',\n",
       " 908: '908',\n",
       " 909: '909',\n",
       " 910: '910',\n",
       " 911: '911',\n",
       " 912: '912',\n",
       " 913: '913',\n",
       " 914: '914',\n",
       " 915: '915',\n",
       " 916: '916',\n",
       " 917: '917',\n",
       " 918: '918',\n",
       " 919: '919',\n",
       " 920: '920',\n",
       " 921: '921',\n",
       " 922: '922',\n",
       " 923: '923',\n",
       " 924: '924',\n",
       " 925: '925',\n",
       " 926: '926',\n",
       " 927: '927',\n",
       " 928: '928',\n",
       " 929: '929',\n",
       " 930: '930',\n",
       " 931: '931',\n",
       " 932: '932',\n",
       " 933: '933',\n",
       " 934: '934',\n",
       " 935: '935',\n",
       " 936: '936',\n",
       " 937: '937',\n",
       " 938: '938',\n",
       " 939: '939',\n",
       " 940: '940',\n",
       " 941: '941',\n",
       " 942: '942',\n",
       " 943: '943',\n",
       " 944: '944',\n",
       " 945: '945',\n",
       " 946: '946',\n",
       " 947: '947',\n",
       " 948: '948',\n",
       " 949: '949',\n",
       " 950: '950',\n",
       " 951: '951',\n",
       " 952: '952',\n",
       " 953: '953',\n",
       " 954: '954',\n",
       " 955: '955',\n",
       " 956: '956',\n",
       " 957: '957',\n",
       " 958: '958',\n",
       " 959: '959',\n",
       " 960: '960',\n",
       " 961: '961',\n",
       " 962: '962',\n",
       " 963: '963',\n",
       " 964: '964',\n",
       " 965: '965',\n",
       " 966: '966',\n",
       " 967: '967',\n",
       " 968: '968',\n",
       " 969: '969',\n",
       " 970: '970',\n",
       " 971: '971',\n",
       " 972: '972',\n",
       " 973: '973',\n",
       " 974: '974',\n",
       " 975: '975',\n",
       " 976: '976',\n",
       " 977: '977',\n",
       " 978: '978',\n",
       " 979: '979',\n",
       " 980: '980',\n",
       " 981: '981',\n",
       " 982: '982',\n",
       " 983: '983',\n",
       " 984: '984',\n",
       " 985: '985',\n",
       " 986: '986',\n",
       " 987: '987',\n",
       " 988: '988',\n",
       " 989: '989',\n",
       " 990: '990',\n",
       " 991: '991',\n",
       " 992: '992',\n",
       " 993: '993',\n",
       " 994: '994',\n",
       " 995: '995',\n",
       " 996: '996',\n",
       " 997: '997',\n",
       " 998: '998',\n",
       " 999: '999',\n",
       " ...}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8916\n"
     ]
    }
   ],
   "source": [
    "for prediction in predicts[0]:\n",
    "    val = idx_to_vocab[prediction[0]]\n",
    "    print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13g7tDjWgtXV"
   },
   "source": [
    "## 5 Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddO26OT-g_QK"
   },
   "source": [
    "The predict process is similar to the *predict* of the Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9iHL6tmaL_j"
   },
   "outputs": [],
   "source": [
    "from data import preproc as pp\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# predict() function will return the predicts with the probabilities\n",
    "predicts, _ = model.predict(x=dtgen.next_test_batch(),\n",
    "                            steps=dtgen.steps['test'],\n",
    "                            ctc_decode=True,\n",
    "                            verbose=1)\n",
    "\n",
    "# decode to string\n",
    "# predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
    "# ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JcAs3Q3WNJ-"
   },
   "source": [
    "## 6 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LuZBRepWbom"
   },
   "source": [
    "Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n",
    "\n",
    "* predicts\n",
    "* ground_truth\n",
    "* norm_accentuation (calculation with/without accentuation)\n",
    "* norm_punctuation (calculation with/without punctuation marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gCwEYdKWOPK"
   },
   "outputs": [],
   "source": [
    "from data import evaluation\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n",
    "\n",
    "e_corpus = \"\\n\".join([\n",
    "    f\"Total test images:    {dtgen.size['test']}\",\n",
    "    f\"Total time:           {total_time}\",\n",
    "    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n",
    "    f\"Metrics:\",\n",
    "    f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
    "    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
    "    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
    "    lg.write(e_corpus)\n",
    "    print(e_corpus)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oMty1YwuWHpN"
   ],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "60890892eea6c86700db8a9ea51fde16553fd89a361df4a47f1b0097d87b1a20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
