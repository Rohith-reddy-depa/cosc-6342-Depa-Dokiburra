{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/arthurflor23/handwritten-text-recognition/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-v0E_S-mQP"
   },
   "source": [
    "<img src=\"https://github.com/arthurflor23/handwritten-text-recognition/blob/master/doc/image/header.png?raw=true\" />\n",
    "\n",
    "# Handwritten Text Recognition using TensorFlow 2.x\n",
    "\n",
    "This tutorial shows how you can use the project [Handwritten Text Recognition](https://github.com/arthurflor23/handwritten-text-recognition) in your Google Colab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMty1YwuWHpN"
   },
   "source": [
    "## 1 Localhost Environment\n",
    "\n",
    "We'll make sure you have the project in your Google Drive with the datasets in HDF5. If you already have structured files in the cloud, skip this step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39blvPTPQJpt"
   },
   "source": [
    "### 1.1 Datasets\n",
    "\n",
    "The datasets that you can use:\n",
    "\n",
    "a. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n",
    "\n",
    "b. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n",
    "\n",
    "c. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n",
    "\n",
    "d. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n",
    "\n",
    "e. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVBGMLifWQwl"
   },
   "source": [
    "### 1.2 Raw folder\n",
    "\n",
    "On localhost, download the code project from GitHub and extract the chosen dataset (or all if you prefer) in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n",
    "\n",
    "```\n",
    ".\n",
    "├── raw\n",
    "│   ├── bentham\n",
    "│   │   ├── BenthamDatasetR0-GT\n",
    "│   │   └── BenthamDatasetR0-Images\n",
    "│   ├── iam\n",
    "│   │   ├── ascii\n",
    "│   │   ├── forms\n",
    "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
    "│   │   ├── lines\n",
    "│   │   └── xml\n",
    "│   ├── rimes\n",
    "│   │   ├── eval_2011\n",
    "│   │   ├── eval_2011_annotated.xml\n",
    "│   │   ├── training_2011\n",
    "│   │   └── training_2011.xml\n",
    "│   ├── saintgall\n",
    "│   │   ├── data\n",
    "│   │   ├── ground_truth\n",
    "│   │   ├── README.txt\n",
    "│   │   └── sets\n",
    "│   └── washington\n",
    "│       ├── data\n",
    "│       ├── ground_truth\n",
    "│       ├── README.txt\n",
    "│       └── sets\n",
    "└── src\n",
    "    ├── data\n",
    "    │   ├── evaluation.py\n",
    "    │   ├── generator.py\n",
    "    │   ├── preproc.py\n",
    "    │   ├── reader.py\n",
    "    │   ├── similar_error_analysis.py\n",
    "    ├── main.py\n",
    "    ├── network\n",
    "    │   ├── architecture.py\n",
    "    │   ├── layers.py\n",
    "    │   ├── model.py\n",
    "    └── tutorial.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "After that, create virtual environment and install the dependencies with python 3 and pip:\n",
    "\n",
    "> ```python -m venv .venv && source .venv/bin/activate```\n",
    "\n",
    "> ```pip install -r requirements.txt```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyLRbAwsWSYA"
   },
   "source": [
    "### 1.3 HDF5 files\n",
    "\n",
    "Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n",
    "\n",
    "> ```python main.py --source=<DATASET_NAME> --transform```\n",
    "\n",
    "Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n",
    "\n",
    "\n",
    "```\n",
    ".\n",
    "├── data\n",
    "│   ├── bentham.hdf5\n",
    "│   ├── iam.hdf5\n",
    "│   ├── rimes.hdf5\n",
    "│   ├── saintgall.hdf5\n",
    "│   └── washington.hdf5\n",
    "├── raw\n",
    "│   ├── bentham\n",
    "│   │   ├── BenthamDatasetR0-GT\n",
    "│   │   └── BenthamDatasetR0-Images\n",
    "│   ├── iam\n",
    "│   │   ├── ascii\n",
    "│   │   ├── forms\n",
    "│   │   ├── largeWriterIndependentTextLineRecognitionTask\n",
    "│   │   ├── lines\n",
    "│   │   └── xml\n",
    "│   ├── rimes\n",
    "│   │   ├── eval_2011\n",
    "│   │   ├── eval_2011_annotated.xml\n",
    "│   │   ├── training_2011\n",
    "│   │   └── training_2011.xml\n",
    "│   ├── saintgall\n",
    "│   │   ├── data\n",
    "│   │   ├── ground_truth\n",
    "│   │   ├── README.txt\n",
    "│   │   └── sets\n",
    "│   └── washington\n",
    "│       ├── data\n",
    "│       ├── ground_truth\n",
    "│       ├── README.txt\n",
    "│       └── sets\n",
    "└── src\n",
    "    ├── data\n",
    "    │   ├── evaluation.py\n",
    "    │   ├── generator.py\n",
    "    │   ├── preproc.py\n",
    "    │   ├── reader.py\n",
    "    │   ├── similar_error_analysis.py\n",
    "    ├── main.py\n",
    "    ├── network\n",
    "    │   ├── architecture.py\n",
    "    │   ├── layers.py\n",
    "    │   ├── model.py\n",
    "    └── tutorial.ipynb\n",
    "\n",
    "```\n",
    "\n",
    "Then upload the **data** and **src** folders in the same directory in your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jydsAcWgWVth"
   },
   "source": [
    "## 2 Google Drive Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk3e7YJiXzSl"
   },
   "source": [
    "### 2.1 TensorFlow 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7twXyNGXtbJ"
   },
   "source": [
    "Make sure the jupyter notebook is using GPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHw4tODULT1Z"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMg-B5PH9h3r"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name != \"/device:GPU:0\":\n",
    "    raise SystemError(\"GPU device not found\")\n",
    "\n",
    "print(\"Found GPU at: {}\".format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyMv5wyDXxqc"
   },
   "source": [
    "### 2.2 Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5gj6qwoX9W3"
   },
   "source": [
    "Mount your Google Drive partition.\n",
    "\n",
    "**Note:** *\\\"Colab Notebooks/handwritten-text-recognition/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ACQn1iBF9k9O"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"./gdrive\", force_remount=True)\n",
    "\n",
    "%cd \"./gdrive/My Drive/Colab Notebooks/handwritten-text-recognition/src/\"\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwogUA8RZAyp"
   },
   "source": [
    "After mount, you can see the list os files in the project folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-fj7fSngY1IX"
   },
   "source": [
    "## 3 Set Python Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p6Q4cOlWhNl3"
   },
   "source": [
    "### 3.1 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvqL2Eq5ZUc7"
   },
   "source": [
    "First, let's define our environment variables.\n",
    "\n",
    "Set the main configuration parameters, like input size, batch size, number of epochs and list of characters. This make compatible with **main.py** and jupyter notebook:\n",
    "\n",
    "* **dataset**: \"bentham\", \"iam\", \"rimes\", \"saintgall\", \"washington\"\n",
    "\n",
    "* **arch**: network to run: \"bluche\", \"puigcerver\", \"flor\"\n",
    "\n",
    "* **epochs**: number of epochs\n",
    "\n",
    "* **batch_size**: number size of the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_Qpr3drnGMWS"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from data import preproc as pp\n",
    "\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 16\n",
    "ARCH = \"flor\"\n",
    "\n",
    "IMG_SIZE = (1642,6986, 1)\n",
    "DATA_ROOT_PATH = \"..//data\"\n",
    "IMAGES_PATH = os.path.join(DATA_ROOT_PATH, \"images\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642 6986\n"
     ]
    }
   ],
   "source": [
    "maxh=0\n",
    "maxw=0\n",
    "for i in os.listdir(IMAGES_PATH):\n",
    "    l=os.listdir(IMAGES_PATH+\"/\"+i)\n",
    "    l.pop()\n",
    "    for j in l:\n",
    "        for k in os.listdir(IMAGES_PATH+\"/\"+i+\"/\"+j):\n",
    "            \n",
    "            if \"jpg\" in k:\n",
    "                ime=Image.open(IMAGES_PATH+\"/\"+i+\"/\"+j+\"/\"+k)\n",
    "                maxh=max(maxh,ime.height)\n",
    "                maxw=max(maxw,ime.width)\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "print(maxh,maxw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EsU8fQR2jMrP"
   },
   "outputs": [],
   "source": [
    "vocab = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_voc.txt\")) as f:\n",
    "  vocab = f.readlines()\n",
    "\n",
    "idx_to_vocab = {i:value.strip() for i, value in enumerate(vocab)}\n",
    "vocab_to_idx = {value:key for key, value in idx_to_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DrpcgsidjMrP"
   },
   "outputs": [],
   "source": [
    "train_data = None\n",
    "\n",
    "with open(os.path.join(DATA_ROOT_PATH, \"new_train.txt\"), encoding=\"latin1\") as f:\n",
    "  train_data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFextshOhTKr"
   },
   "source": [
    "### 3.2 DataGenerator Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfZ1mfvsanu1"
   },
   "source": [
    "The second class is **DataGenerator()**, responsible for:\n",
    "\n",
    "* Load the dataset partitions (train, valid, test);\n",
    "\n",
    "* Manager batchs for train/validation/test process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "8k9vpNzMIAi2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "class DataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data, img_size, batch_size, mode=\"TRAIN\"):\n",
    "        self.data = data\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        end = (i+1) * self.batch_size\n",
    "        \n",
    "        \n",
    "        h=0\n",
    "        w=0\n",
    "        \n",
    "        for ii, df_index in enumerate(range(start, end)):\n",
    "            curr_data = self.data[ii].split()\n",
    "            curr_img_path = curr_data[0]\n",
    "            curr_label = vocab_to_idx[curr_data[1]]\n",
    "\n",
    "            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "            curr_img = Image.open(curr_img_path)\n",
    "            h=max(h,curr_img.height)\n",
    "            w=max(w,curr_img.width)\n",
    "            self.img_size=(h,w,1)\n",
    "        \n",
    "        batch_images = np.zeros((self.batch_size, self.img_size[0], self.img_size[1], 1))\n",
    "        batch_labels = np.zeros((self.batch_size, 1))\n",
    "        \n",
    "        for ii, df_index in enumerate(range(start, end)):\n",
    "            curr_data = self.data[ii].split()\n",
    "            curr_img_path = curr_data[0]\n",
    "            curr_label = vocab_to_idx[curr_data[1]]\n",
    "\n",
    "            curr_img_path = \"/\".join(curr_img_path.split(\"/\")[2:])\n",
    "            curr_img_path = os.path.join(IMAGES_PATH, curr_img_path)\n",
    "\n",
    "            curr_img = Image.open(curr_img_path)\n",
    "            curr_img = ImageOps.grayscale(curr_img)\n",
    "            \n",
    "            \n",
    "            \n",
    "            delta_width = w - curr_img.width\n",
    "            delta_height = h - curr_img.height\n",
    "            pad_width = delta_width // 2\n",
    "            pad_height = delta_height // 2\n",
    "            padding = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "            curr_img = ImageOps.expand(curr_img, padding)\n",
    "            \n",
    "            curr_img = img_to_array(curr_img)\n",
    "            img_shape = curr_img.shape\n",
    "            #curr_img = tf.image.resize(curr_img, (self.img_size[0], self.img_size[1]), method=\"nearest\")\n",
    "            #curr_img = curr_img.numpy().reshape((self.img_size[0], self.img_size[1]))\n",
    "            curr_img = pp.preprocess(curr_img_path, self.img_size)\n",
    "            batch_images[ii, :, :, 0] = curr_img / 255.\n",
    "            batch_labels[ii, :] = curr_label\n",
    "                    \n",
    "        if self.mode == \"TRAIN\":\n",
    "          return batch_images, batch_labels\n",
    "        else:\n",
    "          return batch_images\n",
    "        \n",
    "    def __len__(self):\n",
    "      return len(self.data) // self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "uxhnYiJUjMrR"
   },
   "outputs": [],
   "source": [
    "train_datagen = DataGen(train_data, IMG_SIZE, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OdgNLK0hYAA"
   },
   "source": [
    "### 3.3 HTRModel Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHktk8AFcnKy"
   },
   "source": [
    "The third class is **HTRModel()**, was developed to be easy to use and to abstract the complicated flow of a HTR system. It's responsible for:\n",
    "\n",
    "* Create model with Handwritten Text Recognition flow, in which calculate the loss function by CTC and decode output to calculate the HTR metrics (CER, WER and SER);\n",
    "\n",
    "* Save and load model;\n",
    "\n",
    "* Load weights in the models (train/infer);\n",
    "\n",
    "* Make Train/Predict process using *generator*.\n",
    "\n",
    "To make a dynamic HTRModel, its parameters are the *architecture*, *input_size* and *vocab_size*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nV0GreStISTR"
   },
   "outputs": [],
   "source": [
    "from network.model import HTRModel\n",
    "\n",
    "# create and compile HTRModel\n",
    "model = HTRModel(architecture=ARCH,\n",
    "                 input_size=IMG_SIZE,\n",
    "                 vocab_size=11031,\n",
    "                 beam_width=10,\n",
    "                 stop_tolerance=20,\n",
    "                 reduce_tolerance=15,\n",
    "                 reduce_factor=0.1)\n",
    "\n",
    "model.compile(learning_rate=0.001)\n",
    "#model.summary(output_path, \"summary.txt\")\n",
    "\n",
    "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
    "#model.load_checkpoint(target=target_path)\n",
    "\n",
    "#callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T1fnz0Eugqru"
   },
   "source": [
    "## 4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1mLOcqYgsO-"
   },
   "source": [
    "The training process is similar to the *fit()* of the Keras. After training, the information (epochs and minimum loss) is save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2P6MSoxCISlD",
    "outputId": "4f21673d-a31a-4a07-b65f-0e0aae4a688d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model_4/reshape_4/Reshape' defined at (most recent call last):\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n      app.start()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n      yield self.process_one()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n      self.ctx_run(self.run)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-23-3bea180d8640>\", line 4, in <module>\n      h = model.fit(x=train_datagen,\n    File \"C:\\Users\\pavan\\Documents\\4 th yr\\btechp\\ocr\\src\\network\\model.py\", line 195, in fit\n      out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 137, in call\n      result = tf.reshape(inputs, (tf.shape(inputs)[0],) + self.target_shape)\nNode: 'model_4/reshape_4/Reshape'\nInput to reshape is a tensor with 359936 values, but the requested shape has 131072\n\t [[{{node model_4/reshape_4/Reshape}}]] [Op:__inference_train_function_27269]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3bea180d8640>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#start_time = datetime.datetime.now()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m h = model.fit(x=train_datagen,\n\u001b[0m\u001b[0;32m      5\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m               verbose=1)\n",
      "\u001b[1;32m~\\Documents\\4 th yr\\btechp\\ocr\\src\\network\\model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n\u001b[0m\u001b[0;32m    196\u001b[0m                              \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m                              \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_4/reshape_4/Reshape' defined at (most recent call last):\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n      app.start()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 612, in start\n      self.io_loop.start()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\asyncio\\events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 688, in <lambda>\n      lambda f: self._run_callback(functools.partial(callback, future))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 741, in _run_callback\n      ret = callback()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 814, in inner\n      self.ctx_run(self.run)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 381, in dispatch_queue\n      yield self.process_one()\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 250, in wrapper\n      runner = Runner(ctx_run, result, future, yielded)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 741, in __init__\n      self.ctx_run(self.run)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 775, in run\n      yielded = self.gen.send(value)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 365, in process_one\n      yield gen.maybe_future(dispatch(*args))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n      yield gen.maybe_future(handler(stream, idents, msg))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 543, in execute_request\n      self.do_execute(\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 234, in wrapper\n      yielded = ctx_run(next, result)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 306, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2894, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2940, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3165, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3437, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-23-3bea180d8640>\", line 4, in <module>\n      h = model.fit(x=train_datagen,\n    File \"C:\\Users\\pavan\\Documents\\4 th yr\\btechp\\ocr\\src\\network\\model.py\", line 195, in fit\n      out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n      y_pred = self(x, training=True)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\pavan\\anaconda3\\lib\\site-packages\\keras\\layers\\reshaping\\reshape.py\", line 137, in call\n      result = tf.reshape(inputs, (tf.shape(inputs)[0],) + self.target_shape)\nNode: 'model_4/reshape_4/Reshape'\nInput to reshape is a tensor with 359936 values, but the requested shape has 131072\n\t [[{{node model_4/reshape_4/Reshape}}]] [Op:__inference_train_function_27269]"
     ]
    }
   ],
   "source": [
    "# to calculate total and average time per epoch\n",
    "#start_time = datetime.datetime.now()\n",
    "\n",
    "h = model.fit(x=train_datagen,\n",
    "              epochs=EPOCHS,\n",
    "              verbose=1)\n",
    "\n",
    "'''\n",
    "total_time = datetime.datetime.now() - start_time\n",
    "\n",
    "loss = h.history['loss']\n",
    "val_loss = h.history['val_loss']\n",
    "\n",
    "min_val_loss = min(val_loss)\n",
    "min_val_loss_i = val_loss.index(min_val_loss)\n",
    "\n",
    "time_epoch = (total_time / len(loss))\n",
    "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
    "\n",
    "t_corpus = \"\\n\".join([\n",
    "    f\"Total train images:      {dtgen.size['train']}\",\n",
    "    f\"Total validation images: {dtgen.size['valid']}\",\n",
    "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
    "    f\"Total time:              {total_time}\",\n",
    "    f\"Time per epoch:          {time_epoch}\",\n",
    "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
    "    f\"Total epochs:            {len(loss)}\",\n",
    "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
    "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
    "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n",
    "    lg.write(t_corpus)\n",
    "    print(t_corpus)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13g7tDjWgtXV"
   },
   "source": [
    "## 5 Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddO26OT-g_QK"
   },
   "source": [
    "The predict process is similar to the *predict* of the Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9iHL6tmaL_j"
   },
   "outputs": [],
   "source": [
    "from data import preproc as pp\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# predict() function will return the predicts with the probabilities\n",
    "predicts, _ = model.predict(x=dtgen.next_test_batch(),\n",
    "                            steps=dtgen.steps['test'],\n",
    "                            ctc_decode=True,\n",
    "                            verbose=1)\n",
    "\n",
    "# decode to string\n",
    "predicts = [dtgen.tokenizer.decode(x[0]) for x in predicts]\n",
    "ground_truth = [x.decode() for x in dtgen.dataset['test']['gt']]\n",
    "\n",
    "total_time = datetime.datetime.now() - start_time\n",
    "\n",
    "# mount predict corpus file\n",
    "with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n",
    "    for pd, gt in zip(predicts, ground_truth):\n",
    "        lg.write(f\"TE_L {gt}\\nTE_P {pd}\\n\")\n",
    "   \n",
    "for i, item in enumerate(dtgen.dataset['test']['dt'][:10]):\n",
    "    print(\"=\" * 1024, \"\\n\")\n",
    "    cv2_imshow(pp.adjust_to_see(item))\n",
    "    print(ground_truth[i])\n",
    "    print(predicts[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JcAs3Q3WNJ-"
   },
   "source": [
    "## 6 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LuZBRepWbom"
   },
   "source": [
    "Evaluation process is more manual process. Here we have the `ocr_metrics`, but feel free to implement other metrics instead. In the function, we have three parameters: \n",
    "\n",
    "* predicts\n",
    "* ground_truth\n",
    "* norm_accentuation (calculation with/without accentuation)\n",
    "* norm_punctuation (calculation with/without punctuation marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gCwEYdKWOPK"
   },
   "outputs": [],
   "source": [
    "from data import evaluation\n",
    "\n",
    "evaluate = evaluation.ocr_metrics(predicts, ground_truth)\n",
    "\n",
    "e_corpus = \"\\n\".join([\n",
    "    f\"Total test images:    {dtgen.size['test']}\",\n",
    "    f\"Total time:           {total_time}\",\n",
    "    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n",
    "    f\"Metrics:\",\n",
    "    f\"Character Error Rate: {evaluate[0]:.8f}\",\n",
    "    f\"Word Error Rate:      {evaluate[1]:.8f}\",\n",
    "    f\"Sequence Error Rate:  {evaluate[2]:.8f}\"\n",
    "])\n",
    "\n",
    "with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n",
    "    lg.write(e_corpus)\n",
    "    print(e_corpus)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oMty1YwuWHpN"
   ],
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "vscode": {
   "interpreter": {
    "hash": "60890892eea6c86700db8a9ea51fde16553fd89a361df4a47f1b0097d87b1a20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
